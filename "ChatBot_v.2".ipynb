{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"P100.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3e3ba19b4624023b258b885f6ed6938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_efbe73a0f81f4a25b0d5d889fa14b53d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_70d1948373a1445990e76184c6cdbfff",
              "IPY_MODEL_0e1d44eda96544bb91392f7fb839d6a9"
            ]
          }
        },
        "efbe73a0f81f4a25b0d5d889fa14b53d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70d1948373a1445990e76184c6cdbfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ab766faa9134b5c8d2638be949b67bc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 674,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 674,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a70836a532341948437f427a8fad21f"
          }
        },
        "0e1d44eda96544bb91392f7fb839d6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b636cfa41e744e6aa69aff1eede510a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 674/674 [00:00&lt;00:00, 14.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71b673ba7dc64ed2ae9d68008e2af7cd"
          }
        },
        "6ab766faa9134b5c8d2638be949b67bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a70836a532341948437f427a8fad21f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b636cfa41e744e6aa69aff1eede510a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71b673ba7dc64ed2ae9d68008e2af7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9677265fe09f41d992a47917e4f9b480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1675e6e91cf743e199f1dc28e405df21",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3fda4809a78945998cf7428b75daebd3",
              "IPY_MODEL_6fe445b1e6e546f39e4220c05ccad446"
            ]
          }
        },
        "1675e6e91cf743e199f1dc28e405df21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fda4809a78945998cf7428b75daebd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cdb6337229484e65b2fc6bbb1708644b",
            "_dom_classes": [],
            "description": "Downloading:  68%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1730074771,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1173129216,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00f52b1676d244ac9da35bc2d914a9c9"
          }
        },
        "6fe445b1e6e546f39e4220c05ccad446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5afef2e30a2043268a1364c0e7fe442c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.17G/1.73G [00:24&lt;00:09, 56.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d549d17e0784f2c891d5c16584097cb"
          }
        },
        "cdb6337229484e65b2fc6bbb1708644b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00f52b1676d244ac9da35bc2d914a9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5afef2e30a2043268a1364c0e7fe442c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d549d17e0784f2c891d5c16584097cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LongaBonga/ChatBot-RUGPT-3/blob/main/%22ChatBot_v.2%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbYXou6chZf",
        "outputId": "36e3ebe0-29e4-421a-8a01-5f6126ea5459"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 12 12:33:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZM84PmWO9l_",
        "outputId": "7d0c6eb7-fec9-4376-f710-4ff186458fb1"
      },
      "source": [
        "!pip3 install -U gitpython\n",
        "!pip3 install transformers\n",
        "!pip3 install urllib3\n",
        "!pip3 install gputil\n",
        "!pip3 install tabulate"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\r\u001b[K     |██                              | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 31.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40kB 24.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 61kB 25.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 81kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 102kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 112kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 122kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 133kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 143kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 153kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 17.7MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 31.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 35.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 40kB 38.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 38.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 61kB 39.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.7 gitpython-3.1.14 smmap-4.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 20.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=7b86cb5c4c70e9afa82a4ecddea4aad1fd27f525bd3737f152fc0b8aff06d26c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (1.24.3)\n",
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=77ae21a0c471b7a00ae795ed7dfa99d00d3f99a4ac74af77679bfa8944cdec6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOOwCgudPMLO",
        "outputId": "a582fb15-d204-47d1-c2ea-fe4d3af6fb10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tdDgWmyPMOq"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "data = pd.read_csv('drive/MyDrive/dialogues.tsv', sep='\\t')\n",
        "data = data.drop(9635)\n",
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlamgI3MPMS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58ebf47-f432-4093-a96c-bd4aa57defe2"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "import argparse\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print('GPU')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "a3e3ba19b4624023b258b885f6ed6938",
            "efbe73a0f81f4a25b0d5d889fa14b53d",
            "70d1948373a1445990e76184c6cdbfff",
            "0e1d44eda96544bb91392f7fb839d6a9",
            "6ab766faa9134b5c8d2638be949b67bc",
            "3a70836a532341948437f427a8fad21f",
            "b636cfa41e744e6aa69aff1eede510a0",
            "71b673ba7dc64ed2ae9d68008e2af7cd",
            "9677265fe09f41d992a47917e4f9b480",
            "1675e6e91cf743e199f1dc28e405df21",
            "3fda4809a78945998cf7428b75daebd3",
            "6fe445b1e6e546f39e4220c05ccad446",
            "cdb6337229484e65b2fc6bbb1708644b",
            "00f52b1676d244ac9da35bc2d914a9c9",
            "5afef2e30a2043268a1364c0e7fe442c",
            "7d549d17e0784f2c891d5c16584097cb"
          ]
        },
        "id": "MfG6tKEuPMVx",
        "outputId": "178e675b-97a0-47e0-f316-ecd0441225ae"
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3e3ba19b4624023b258b885f6ed6938",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=674.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9677265fe09f41d992a47917e4f9b480",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1730074771.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F5kdARwPMfh"
      },
      "source": [
        "SPECIAL_TOKENS = {'bos_token' : \"<bos>\", \"eos_token\" :\"<eos>\", 'additional_special_tokens': [\"<speaker1>\", \"<speaker2>\", \"<fos>\"]}\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uz7Msz5PMm-"
      },
      "source": [
        "def merge_replyes(buffer):\n",
        "    history = []\n",
        "    i = 0\n",
        "    a = 0\n",
        "    b = a\n",
        "    while (i < len(buffer)):\n",
        "        same = ''\n",
        "        a = i\n",
        "        b = a\n",
        "        while (i < len(buffer) and b < len(buffer) and buffer[a][0] == buffer[b][0]):\n",
        "            b += 1\n",
        "        \n",
        "        for k in range(a, b):\n",
        "            same += buffer[k][1] + '. '\n",
        "        history.append([buffer[a][0], same])\n",
        "        i = b\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvDk4ptjPMrN"
      },
      "source": [
        "import re\n",
        "import random\n",
        "def generate_example(row):\n",
        "    parsed = re.findall(r'Пользователь [12].+?(?=</span>)', data['dialogue'][row].replace(\"<br />\", \" \"))\n",
        "    history = merge_replyes([[1 if sentenses[13] == '1' else 2, sentenses[16:]] for sentenses in parsed])\n",
        "    # print(history)\n",
        "    random_slice = random.randint(1, min(len(history) - 1, 18))   # MAKE MAX 20 SENTENSES\n",
        "    \n",
        "    if history[random_slice][0] == 1:\n",
        "        persona = re.findall(r'.+?(?=</span>)', data['persona_2_profile'][row].replace(\"<br />\", \" \"))[0][26:]\n",
        "    else:\n",
        "        persona = re.findall(r'.+?(?=</span>)', data['persona_1_profile'][row].replace(\"<br />\", \" \"))[0][26:]\n",
        "        \n",
        "    reply = history[random_slice][1]\n",
        "    history = [[i[1]] for i in history[:random_slice]]\n",
        "    \n",
        "    \n",
        "    return history, [reply], persona.split()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs7O8AjYk_Rn"
      },
      "source": [
        "generate_example(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs80a5l5PMuM"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "persona = [[\"Я\", \"люблю\", \"играть\", \"в\", \"баскетбол\", \".\"],\n",
        "           [\"Я\", \"студент\", \"из\", \"Москвы\", \".\"]]\n",
        "\n",
        "history = [[\"привет\", \"как\", \"Дела\", \"?\"],\n",
        "           [\"Все\", \"хорошо\", \"спасибо\", \".\"]]\n",
        "\n",
        "reply = [\"Рад\", \"это\", \"слышать\"]\n",
        "\n",
        "bos, eos, speaker1, speaker2, fos = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<fos>\"\n",
        "\n",
        "def build_inputs(persona, history, reply):\n",
        "    # Build our sequence by adding delimiters and concatenating\n",
        "    sequence = [[bos] + list(chain(*persona))] + history +  [[fos] + reply + [eos]]\n",
        "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
        "                                for i, s in enumerate(sequence[1:])]\n",
        "    # Build our word, segments and position inputs from the sequence\n",
        "    words = list(chain(*sequence))                          # word tokens\n",
        "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
        "                for i, s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))                      # position tokens\n",
        "    return words, segments, position, sequence\n",
        "\n",
        "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
        "print(words)\n",
        "print(segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7qaYAqHPMxf"
      },
      "source": [
        "def get_example(index):\n",
        "  history , reply, persona = generate_example(index)\n",
        "  words, segments, position, sequence = build_inputs([persona], history, reply)\n",
        "  # print(words)\n",
        "  words = tokenizer(words)['input_ids']\n",
        "  segments = tokenizer(segments)['input_ids']\n",
        "\n",
        "  not_ansewer_tokens_len = len(list(chain(*words[:-len(sequence[-1])])))\n",
        "  answer_tokens = list(chain(*words[-len(sequence[-1][1:]):]))\n",
        "  words = list(chain(*words))\n",
        " \n",
        "  # Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
        "  lm_targets = ([-100] * not_ansewer_tokens_len \\\n",
        "              + [-100] + [-100] + answer_tokens[1:])\n",
        "\n",
        "\n",
        "  # Store the position of the last tokens for the next-sentence prediction loss\n",
        "  last_token = len(words) - 1\n",
        "\n",
        "  input_ids = torch.tensor(words, dtype=torch.long)\n",
        "\n",
        "  token_type_ids = torch.tensor(segments, dtype=torch.long)\n",
        "\n",
        "\n",
        "  mc_token_ids = torch.tensor(last_token, dtype=torch.long)\n",
        "\n",
        "\n",
        "  lm_labels = torch.tensor(lm_targets, dtype=torch.long)\n",
        "  # Next-sentence prediction labels\n",
        "  mc_labels = torch.tensor([0], dtype=torch.long)  \n",
        "\n",
        "  return input_ids, lm_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5_RxlOd9F_v",
        "outputId": "661e664c-cc8f-4b2b-ad65-49bcd9f9bc8d"
      },
      "source": [
        "get_example(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([50258,  1358,  1431,   283,    18, 10150, 12195, 15862,  6890,  8291,\n",
              "           354,    18,  1140, 24888,  1719,   283,   266,   979, 33784,   297,\n",
              "            18,  3693,   724,  1719,  2592, 12270,    18,  5218,   623,   267,\n",
              "          1076,  2844,    18, 50261, 37954,  3715,    23,    18,   225, 50260,\n",
              "          1295,  8398,     5,   985,  1304,  9140,    16,  4723,   387,    35,\n",
              "            18,   225, 50261, 26174,  9140, 23646,    16,   367,  1304,    35,\n",
              "           629,   281, 30407,  2001, 42983,  2356,   269,   376, 49791,    16,\n",
              "          1314, 29393,   393,   516, 25710,   379,  4934,  4337,  1427,    18,\n",
              "          1012,   481, 48107, 11331,   889, 24636,    16,   284,  2078,    30,\n",
              "           816,    18,   439,   387,  4723,    35,    18,   225, 50260, 50262,\n",
              "         26174,  9140,  6816,    16,   417,   411, 36388,    18, 42148,  1427,\n",
              "          2002,  5065,   322,  1196,  5656,    18,  1397,  3730, 23554,   309,\n",
              "         27766,  2240, 11331,   629,    16,  4818,    16, 13277,   282,  3157,\n",
              "           477,  3319, 48111,    18,  1606, 10441, 25159,   282,  8995,   498,\n",
              "           225, 50259]),\n",
              " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "         26174,  9140,  6816,    16,   417,   411, 36388,    18, 42148,  1427,\n",
              "          2002,  5065,   322,  1196,  5656,    18,  1397,  3730, 23554,   309,\n",
              "         27766,  2240, 11331,   629,    16,  4818,    16, 13277,   282,  3157,\n",
              "           477,  3319, 48111,    18,  1606, 10441, 25159,   282,  8995,   498,\n",
              "           225, 50259]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJJSjqZmPM1j"
      },
      "source": [
        "def prepare_input(persona, history):\n",
        "    sequence = [[bos] + list(chain(*persona))] + history\n",
        "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
        "                                for i, s in enumerate(sequence[1:])]\n",
        "    # Build our word, segments and position inputs from the sequence\n",
        "    words = list(chain(*sequence))                          # word tokens\n",
        "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
        "                for i, s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))                      # position tokens            # position tokens\n",
        "\n",
        "    print(words)\n",
        "    words = tokenizer(words)['input_ids']\n",
        "    words = list(chain(*words))\n",
        "    input_ids = torch.tensor([words], dtype=torch.long)\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5j8rmJPM80"
      },
      "source": [
        "class Dialoges(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file):\n",
        "        data = pd.read_csv(csv_file, sep='\\t')\n",
        "        data = data.drop(9635)\n",
        "        data = data.reset_index(drop=True)\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return get_example(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFm3VRQUPNCn"
      },
      "source": [
        "dataset = Dialoges('drive/MyDrive/dialogues.tsv')\n",
        "\n",
        "train_loader = DataLoader(dataset,\n",
        "                          batch_size=1, \n",
        "                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9DMgKyqPjMt"
      },
      "source": [
        "num_epochs = 3\n",
        "learning_rate = 0.0000001\n",
        "warmup_steps = 50\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJNxENqHPjQN"
      },
      "source": [
        "import GPUtil\n",
        "from tabulate import tabulate\n",
        "\n",
        "def train(model, loader, optimizer, scheduler, last_n_losses=200, verbose=True):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    progress_bar = tqdm(total=len(loader.dataset), disable=not verbose, desc='Train', position=0, leave=True)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    cnt = 0\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        outputs = model(input_ids = X, labels = y)\n",
        "        loss = outputs[0]\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]),\n",
        "                                 perplexity=np.exp(np.mean(losses[-last_n_losses:])))\n",
        "        \n",
        "        progress_bar.update()\n",
        "       \n",
        "\n",
        "        if cnt % 1000 == 0 and cnt > 1000:\n",
        "          save_last_model_path = 'drive/MyDrive/last_model_state_dict.pth'\n",
        "          save_last_optimizer_path = 'drive/MyDrive/last_optimizer_state_dict.pth'\n",
        "          torch.save(model.state_dict(), save_last_model_path)\n",
        "          torch.save(optimizer.state_dict(), save_last_optimizer_path)\n",
        "\n",
        "\n",
        "        if (cnt % 1000 == 0):\n",
        "          # GPU information\n",
        "\n",
        "          print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
        "          gpus = GPUtil.getGPUs()\n",
        "          list_gpus = []\n",
        "          for gpu in gpus:\n",
        "              # get the GPU id\n",
        "              gpu_id = gpu.id\n",
        "              # name of GPU\n",
        "              gpu_name = gpu.name\n",
        "              # get % percentage of GPU usage of that GPU\n",
        "              gpu_load = f\"{gpu.load*100}%\"\n",
        "              # get free memory in MB format\n",
        "              gpu_free_memory = f\"{gpu.memoryFree}MB\"\n",
        "              # get used memory\n",
        "              gpu_used_memory = f\"{gpu.memoryUsed}MB\"\n",
        "              # get total memory\n",
        "              gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
        "              # get GPU temperature in Celsius\n",
        "              gpu_temperature = f\"{gpu.temperature} °C\"\n",
        "              gpu_uuid = gpu.uuid\n",
        "              list_gpus.append((\n",
        "                  gpu_id, gpu_name, gpu_load, gpu_free_memory, gpu_used_memory,\n",
        "                  gpu_total_memory, gpu_temperature, gpu_uuid\n",
        "              ))\n",
        "\n",
        "          print(tabulate(list_gpus, headers=(\"id\", \"name\", \"load\", \"free memory\", \"used memory\", \"total memory\",\n",
        "                                   \"temperature\", \"uuid\")))\n",
        "        cnt += 1\n",
        "\n",
        "    progress_bar.close()\n",
        "    \n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diz-L9GYPNHs"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/last_model_state_dict.pth'))\n",
        "optimizer.load_state_dict(torch.load('drive/MyDrive/last_optimizer_state_dict.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "23F1gMpXPjT5",
        "outputId": "3595ca55-f208-4769-bdc8-323d49658cc9"
      },
      "source": [
        "train_losses = []\n",
        "train_perplexities = []\n",
        "\n",
        "for n_epoch in range(1, num_epochs + 1):\n",
        "    \n",
        "    epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n",
        "    mean_train_loss = np.mean(epoch_train_losses)\n",
        "    \n",
        "    train_losses.extend(epoch_train_losses)\n",
        "    train_perplexities.append(np.exp(mean_train_loss))\n",
        "    \n",
        "    \n",
        "    message = f'Epoch: {n_epoch}\\n'\n",
        "    message += f'Train: loss - {mean_train_loss:.4f} | perplexity - {train_perplexities[-1]:.3f}\\n'  \n",
        "    print(message) \n",
        "    # 1160/10012"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:   0%|          | 1/10012 [00:00<25:36,  6.51it/s, loss=1.68, perplexity=5.38]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  43.0%   7925.0MB       8355.0MB       16280.0MB       39.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  10%|█         | 1002/10012 [02:32<26:10,  5.74it/s, loss=2.44, perplexity=11.5]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  79.0%   3535.0MB       12745.0MB      16280.0MB       68.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  20%|█▉        | 2001/10012 [05:02<17:49,  7.49it/s, loss=2.56, perplexity=12.9]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  0.0%    3529.0MB       12751.0MB      16280.0MB       54.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  30%|██▉       | 3001/10012 [08:59<17:42,  6.60it/s, loss=2.61, perplexity=13.6]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  0.0%    3529.0MB       12751.0MB      16280.0MB       54.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  40%|███▉      | 4002/10012 [14:15<41:07:49, 24.64s/it, loss=2.67, perplexity=14.5]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  3.0%    3529.0MB       12751.0MB      16280.0MB       54.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  50%|████▉     | 5001/10012 [16:49<12:22,  6.74it/s, loss=2.72, perplexity=15.2]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name                  load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------------------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla P100-PCIE-16GB  0.0%    883.0MB        15397.0MB      16280.0MB       54.0 °C        GPU-74642858-7aa9-d838-71ef-babcc3a0a23f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:  50%|█████     | 5044/10012 [18:19<11:10,  7.41it/s, loss=2.74, perplexity=15.5]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-378353a4d405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepoch_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmean_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-4a17577b79b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, scheduler, last_n_losses, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ltfpNMM22Gh"
      },
      "source": [
        "def choose_answer(variants):\n",
        "  for beam_output in variants:\n",
        "    i = tokenizer.decode(beam_output)\n",
        "    # print(i)\n",
        "    if i[-1] in '?.!':\n",
        "      return i\n",
        "  \n",
        "  ans = tokenizer.decode(variants[0]).split('.')\n",
        "  # print('########', ' '.join(ans[:-1]))\n",
        "  return ' '.join(ans[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHr_YDTFYsDg"
      },
      "source": [
        "def speak():\n",
        "  \n",
        "  about = '<bos> Меня Зовут Анна.'\n",
        "\n",
        "  history = []\n",
        "  print('Начните диалог')\n",
        "\n",
        "  while(True):\n",
        "    message = input()\n",
        "    i = 1\n",
        "    history.append([' <speaker1> ' + message])\n",
        "    input_text = about + ''.join(list(chain(*history))) + ' <speaker2> <fos>' # history[-4:]\n",
        "\n",
        "    inds = tokenizer(input_text, return_tensors='pt')['input_ids'].to(device)\n",
        "    beam_outputs = model.generate(\n",
        "          input_ids=inds,\n",
        "          top_p=1, \n",
        "          temperature=1,\n",
        "          max_length= len(inds[0]) + 20, \n",
        "          min_length= len(inds[0]) + 20, \n",
        "          num_beams=5, \n",
        "          no_repeat_ngram_size=2, \n",
        "          num_return_sequences=5, \n",
        "          # early_stopping=True,\n",
        "          eos_token_id = 50259,\n",
        "          pad_token_id = 0,\n",
        "          )\n",
        "\n",
        "    answer = choose_answer(beam_outputs)\n",
        "\n",
        "    history.append([' <speaker2> ' + answer[len(input_text):]])\n",
        "    # print(input_text)\n",
        "    # # print(answer)\n",
        "    print(answer[len(input_text):])\n",
        "    # print(history)\n",
        "\n",
        "\n",
        "\n",
        "# repetition_penalty=5.0,\n",
        "#           top_k=5, \n",
        "#           top_p=0.95, \n",
        "#           temperature=1,\n",
        "#           max_length= len(inds[0]) + 20,\n",
        "#           do_sample=True,\n",
        "#           no_repeat_ngram_size=10,\n",
        "#           pad_token_id = 50259,\n",
        "#           # length_penalty = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqhxv_DoPqgp",
        "outputId": "3f5a525a-69be-4f4f-8fa0-0750b01bfd00"
      },
      "source": [
        "speak()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Начните диалог\n",
            "Привет!\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2> <fos> Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?).\n",
            " Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?).\n",
            "Я Ваня, студент, а тебя как зовут?\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> <fos>Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья?\n",
            "Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья?\n",
            "да, есть девушка\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂. Я люблю готовить и готовить люблю. а ты\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂. Я очень люблю готовить, но пока не работаю\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂. Я очень люблю готовить, но не умею готовить\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂. Я люблю готовить и готовить люблю. У меня\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂. Я очень люблю готовить, но пока не могу\n",
            "######## <bos> Меня Зовут Анна  <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?  Чем увлекаешься в свободное время?)  <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке  А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2> <fos> а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю\n",
            " а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю\n",
            "ты идеальна!\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще. Я\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому стараюсь не есть. У меня\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще. У\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще. Ты\n",
            "<bos> Меня Зовут Анна. <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?. Чем увлекаешься в свободное время?). <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке. А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще. Но\n",
            "######## <bos> Меня Зовут Анна  <speaker1> Привет! <speaker2>  Как тебя зовут? Чем занимаешься? Кем работаешь?  Чем увлекаешься в свободное время?)  <speaker1> Я Ваня, студент, а тебя как зовут? <speaker2> Меня зовут Аня, я работаю в банке  А ты чем любишь заниматься? У тебя есть семья? <speaker1> да, есть девушка <speaker2>  а у меня муж и двое детей 😂  Я люблю готовить и готовить люблю <speaker1> ты идеальна! <speaker2> <fos>Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще\n",
            "Я тоже люблю вкусно покушать, но не люблю сладкое, поэтому не ем его вообще\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9i5DLE5Pqlk"
      },
      "source": [
        "\n",
        "arr = [len(generate_example(i)[0]) for i in range(2000, 2420)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ikZnfCsPqqF"
      },
      "source": [
        "print(np.argmax(arr), max(arr))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnCO6GyJj3zN"
      },
      "source": [
        "np.mean(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gabht5jYkEqi"
      },
      "source": [
        "get_example(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTrOzKtzlyIj"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids('<pad>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo3K-eNzPTbz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}