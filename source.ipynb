{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"P100.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c1ee8d538d2498090caa11dbee50e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eb53ef9332d64709a174f29ccd09ca2a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c215ca86c6f547c9864e206716e982a8",
              "IPY_MODEL_06dcb866e02e4d9185f448cbc93b4ee0"
            ]
          }
        },
        "eb53ef9332d64709a174f29ccd09ca2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c215ca86c6f547c9864e206716e982a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76cc925fa93842bf81f87febc8a28d9f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1612610,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1612610,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e32050e3a1149cb883014a405475cdd"
          }
        },
        "06dcb866e02e4d9185f448cbc93b4ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58fa6d2f4ed24ff594d8d93d54da4f14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.61M/1.61M [00:00&lt;00:00, 2.58MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52a22a59849a4270b13ef4a1e8a81044"
          }
        },
        "76cc925fa93842bf81f87febc8a28d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e32050e3a1149cb883014a405475cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58fa6d2f4ed24ff594d8d93d54da4f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52a22a59849a4270b13ef4a1e8a81044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e15dbb51bca422cb6ac47cbc9085c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57fbbed4823f4c8bb33d6c8c1f5f62ae",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7464b1b3f9e43289f63eb1371a1abcc",
              "IPY_MODEL_f3667ac8a0f74c2ea6689a63b606964a"
            ]
          }
        },
        "57fbbed4823f4c8bb33d6c8c1f5f62ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7464b1b3f9e43289f63eb1371a1abcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f078d0aa381a423b96d79bf12cc609b0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1270963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1270963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02f8c9522fdd4a71bdd4b49cf96e82c8"
          }
        },
        "f3667ac8a0f74c2ea6689a63b606964a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_797460edc3534d7da3d0b97a7a9c08e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.27M/1.27M [00:00&lt;00:00, 3.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94a3996072a54a7cbef491d30d73910d"
          }
        },
        "f078d0aa381a423b96d79bf12cc609b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02f8c9522fdd4a71bdd4b49cf96e82c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "797460edc3534d7da3d0b97a7a9c08e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94a3996072a54a7cbef491d30d73910d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbYXou6chZf",
        "outputId": "a26d6550-9f99-42f2-86cd-ec52c0d4862b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 31 19:39:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZM84PmWO9l_",
        "outputId": "5135151a-40a6-449b-fefa-d6abc5e04f25"
      },
      "source": [
        "!pip3 install -U gitpython\n",
        "!pip3 install transformers\n",
        "!pip3 install urllib3\n",
        "!pip3 install gputil\n",
        "!pip3 install tabulate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.7MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.7 gitpython-3.1.14 smmap-4.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 12.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f5fee4338619e14fa73db8a2e70c56e985d212d08d52da8a9ac9b94a4a26e44d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (1.24.3)\n",
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=3dca930e963977df6503f5a0c1e79ffb3e0e7debf231d3c039c06d85362eb3b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOOwCgudPMLO",
        "outputId": "df98db4f-c6c4-4e19-cc4f-32401ab66b9e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2tdDgWmyPMOq"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "data = pd.read_csv('drive/MyDrive/dialogues.tsv', sep='\\t')\n",
        "data = data.drop(9635)\n",
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UlamgI3MPMS7",
        "outputId": "36b61ca8-34fe-41db-b432-b9689fa70ef6"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "import argparse\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print('GPU')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfG6tKEuPMVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "cccb75e1318549e28d23cd1ba4b7b452",
            "475e05b81cbb44009ddc4f34d137440d",
            "4c1ee8d538d2498090caa11dbee50e8c",
            "eb53ef9332d64709a174f29ccd09ca2a",
            "c215ca86c6f547c9864e206716e982a8",
            "06dcb866e02e4d9185f448cbc93b4ee0",
            "76cc925fa93842bf81f87febc8a28d9f",
            "8e32050e3a1149cb883014a405475cdd",
            "58fa6d2f4ed24ff594d8d93d54da4f14",
            "52a22a59849a4270b13ef4a1e8a81044",
            "9e15dbb51bca422cb6ac47cbc9085c48",
            "57fbbed4823f4c8bb33d6c8c1f5f62ae",
            "c7464b1b3f9e43289f63eb1371a1abcc",
            "f3667ac8a0f74c2ea6689a63b606964a",
            "f078d0aa381a423b96d79bf12cc609b0",
            "02f8c9522fdd4a71bdd4b49cf96e82c8",
            "797460edc3534d7da3d0b97a7a9c08e4",
            "94a3996072a54a7cbef491d30d73910d"
          ]
        },
        "outputId": "49c41e2d-0222-4b16-f2de-8dd8f5fb23f7"
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:1010: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cccb75e1318549e28d23cd1ba4b7b452",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=674.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "475e05b81cbb44009ddc4f34d137440d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1730074771.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c1ee8d538d2498090caa11dbee50e8c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1612610.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e15dbb51bca422cb6ac47cbc9085c48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1270963.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F5kdARwPMfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be55b75-b790-464f-f194-a2ee84ee932a"
      },
      "source": [
        "SPECIAL_TOKENS = {'bos_token' : \"<bos>\", \"eos_token\" :\"<eos>\", 'additional_special_tokens': [\"<speaker1>\", \"<speaker2>\", \"<fos>\"]}\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50263, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uz7Msz5PMm-"
      },
      "source": [
        "def merge_replyes(buffer):\n",
        "    history = []\n",
        "    i = 0\n",
        "    a = 0\n",
        "    b = a\n",
        "    while (i < len(buffer)):\n",
        "        same = ''\n",
        "        a = i\n",
        "        b = a\n",
        "        while (i < len(buffer) and b < len(buffer) and buffer[a][0] == buffer[b][0]):\n",
        "            b += 1\n",
        "        \n",
        "        for k in range(a, b):\n",
        "            same += buffer[k][1] + '. '\n",
        "        history.append([buffer[a][0], same])\n",
        "        i = b\n",
        "    return history"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvDk4ptjPMrN"
      },
      "source": [
        "import re\n",
        "import random\n",
        "def generate_example(row):\n",
        "    parsed = re.findall(r'Пользователь [12].+?(?=</span>)', data['dialogue'][row].replace(\"<br />\", \" \"))\n",
        "    history = merge_replyes([[1 if sentenses[13] == '1' else 2, sentenses[16:]] for sentenses in parsed])\n",
        "    # print(history)\n",
        "    random_slice = random.randint(1, min(len(history) - 1, 18))   # MAKE MAX 20 SENTENSES\n",
        "    \n",
        "    if history[random_slice][0] == 1:\n",
        "        persona = re.findall(r'.+?(?=</span>)', data['persona_2_profile'][row].replace(\"<br />\", \" \"))[0][26:]\n",
        "    else:\n",
        "        persona = re.findall(r'.+?(?=</span>)', data['persona_1_profile'][row].replace(\"<br />\", \" \"))[0][26:]\n",
        "        \n",
        "    reply = history[random_slice][1]\n",
        "    history = [[i[1]] for i in history[:random_slice]]\n",
        "    \n",
        "    \n",
        "    return history, [reply], persona.split()\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs7O8AjYk_Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de72a16-b92f-4457-acc9-6d5af07382b3"
      },
      "source": [
        "generate_example(2241)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['Хай. '],\n",
              "  ['Доброго времени суток! Рад пообщаться! Что делаете ?. '],\n",
              "  ['Лежу загораю, А Вы,???????. Аууу. Вы где. Вы тоже меня хотите бросить. '],\n",
              "  ['Круто! Вы наверное в путешествии? А я готовлю лекцию . Я преподаю религиезнавство в семинарии. Завтра сложный материал, голова уже кругом. Вот зашёл в чат немного отдохнуть.. '],\n",
              "  ['И так всегда. Я ведь блогер. Обожаю путешествовать!!!. А у бога я не верю. А в бога я не верю. '],\n",
              "  ['Я здесь 😊 просто у меня много кошек, а вечером они любят внимание 😁. Пытаюсь писать, а они отвлекают 😁😁😁. '],\n",
              "  ['Приколямба😹😹😹👍👍👍👍🤟✌️✌️🤟🤟. Хахаха смеююююсь. '],\n",
              "  ['Я бы тоже хотел куда- то поехать. А где Вы сейчас отдыхаете?. ']],\n",
              " ['Обожаю кошек. Я на островах. На Мадагаскаре. А вы чем занимаетесь. Какая у вас группа крови. Какой у вас рост. Я готовлю конфеты. '],\n",
              " ['я',\n",
              "  'преподаватель',\n",
              "  'я',\n",
              "  'работаю',\n",
              "  'в',\n",
              "  'семинарии',\n",
              "  'я',\n",
              "  'живу',\n",
              "  'рядом',\n",
              "  'с',\n",
              "  'работой',\n",
              "  'люблю',\n",
              "  'кошек',\n",
              "  'у',\n",
              "  'меня',\n",
              "  '3',\n",
              "  'брата'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs80a5l5PMuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90b3600-e619-4b33-90ec-9e030e6851bf"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "persona = [[\"Я\", \"люблю\", \"играть\", \"в\", \"баскетбол\", \".\"],\n",
        "           [\"Я\", \"студент\", \"из\", \"Москвы\", \".\"]]\n",
        "\n",
        "history = [[\"привет\", \"как\", \"Дела\", \"?\"],\n",
        "           [\"Все\", \"хорошо\", \"спасибо\", \".\"]]\n",
        "\n",
        "reply = [\"Рад\", \"это\", \"слышать\"]\n",
        "\n",
        "bos, eos, speaker1, speaker2, fos = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<fos>\"\n",
        "\n",
        "def build_inputs(persona, history, reply):\n",
        "    # Build our sequence by adding delimiters and concatenating\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [[fos] + reply + [eos]]\n",
        "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
        "                                for i, s in enumerate(sequence[1:])]\n",
        "    # Build our word, segments and position inputs from the sequence\n",
        "    words = list(chain(*sequence))                          # word tokens\n",
        "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
        "                for i, s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))                      # position tokens\n",
        "    return words, segments, position, sequence\n",
        "\n",
        "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
        "print(words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<bos>', 'Я', 'люблю', 'играть', 'в', 'баскетбол', '.', 'Я', 'студент', 'из', 'Москвы', '.', '<speaker1>', 'привет', 'как', 'Дела', '?', '<speaker2>', 'Все', 'хорошо', 'спасибо', '.', '<speaker1>', '<fos>', 'Рад', 'это', 'слышать', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7qaYAqHPMxf"
      },
      "source": [
        "def get_example(index):\n",
        "  history , reply, persona = generate_example(index)\n",
        "  words, segments, position, sequence = build_inputs([persona], history, reply)\n",
        "  # print(words)\n",
        "  words = tokenizer(words)['input_ids']\n",
        "  segments = tokenizer(segments)['input_ids']\n",
        "\n",
        "  not_ansewer_tokens_len = len(list(chain(*words[:-len(sequence[-1])])))\n",
        "  answer_tokens = list(chain(*words[-len(sequence[-1][1:]):]))\n",
        "  words = list(chain(*words))\n",
        " \n",
        "  # Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
        "  lm_targets = ([-100] * not_ansewer_tokens_len \\\n",
        "              + [-100] + answer_tokens)\n",
        "\n",
        "\n",
        "  # Store the position of the last tokens for the next-sentence prediction loss\n",
        "  last_token = len(words) - 1\n",
        "\n",
        "  input_ids = torch.tensor(words, dtype=torch.long)\n",
        "\n",
        "  token_type_ids = torch.tensor(segments, dtype=torch.long)\n",
        "\n",
        "\n",
        "  mc_token_ids = torch.tensor(last_token, dtype=torch.long)\n",
        "\n",
        "\n",
        "  lm_labels = torch.tensor(lm_targets, dtype=torch.long)\n",
        "  # Next-sentence prediction labels\n",
        "  mc_labels = torch.tensor([0], dtype=torch.long)  \n",
        "\n",
        "  return input_ids, lm_labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJJSjqZmPM1j"
      },
      "source": [
        "def prepare_input(persona, history):\n",
        "    sequence = [[bos] + list(chain(*persona))] + history\n",
        "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
        "                                for i, s in enumerate(sequence[1:])]\n",
        "    # Build our word, segments and position inputs from the sequence\n",
        "    words = list(chain(*sequence))                          # word tokens\n",
        "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
        "                for i, s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))                      # position tokens            # position tokens\n",
        "\n",
        "    print(words)\n",
        "    words = tokenizer(words)['input_ids']\n",
        "    words = list(chain(*words))\n",
        "    input_ids = torch.tensor([words], dtype=torch.long)\n",
        "    return input_ids"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5j8rmJPM80"
      },
      "source": [
        "class Dialoges(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file):\n",
        "        data = pd.read_csv(csv_file, sep='\\t')\n",
        "        data = data.drop(9635)\n",
        "        data = data.reset_index(drop=True)\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return get_example(idx)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFm3VRQUPNCn"
      },
      "source": [
        "dataset = Dialoges('drive/MyDrive/dialogues.tsv')\n",
        "\n",
        "train_loader = DataLoader(dataset,\n",
        "                          batch_size=1, \n",
        "                          shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9DMgKyqPjMt"
      },
      "source": [
        "num_epochs = 3\n",
        "learning_rate = 0.000003\n",
        "warmup_steps = 50\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJNxENqHPjQN"
      },
      "source": [
        "import GPUtil\n",
        "from tabulate import tabulate\n",
        "\n",
        "def train(model, loader, optimizer, scheduler, last_n_losses=200, verbose=True):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    progress_bar = tqdm(total=len(loader.dataset), disable=not verbose, desc='Train', position=0, leave=True)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    cnt = 0\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        outputs = model(input_ids = X, labels = y)\n",
        "        loss = outputs[0]\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]),\n",
        "                                 perplexity=np.exp(np.mean(losses[-last_n_losses:])))\n",
        "        \n",
        "        progress_bar.update()\n",
        "       \n",
        "\n",
        "        if cnt % 1000 == 0 and cnt > 1000:\n",
        "          save_last_model_path = 'drive/MyDrive/last_model_state_dict.pth'\n",
        "          save_last_optimizer_path = 'drive/MyDrive/last_optimizer_state_dict.pth'\n",
        "          torch.save(model.state_dict(), save_last_model_path)\n",
        "          torch.save(optimizer.state_dict(), save_last_optimizer_path)\n",
        "\n",
        "\n",
        "        if (cnt % 1000 == 0):\n",
        "          # GPU information\n",
        "\n",
        "          print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
        "          gpus = GPUtil.getGPUs()\n",
        "          list_gpus = []\n",
        "          for gpu in gpus:\n",
        "              # get the GPU id\n",
        "              gpu_id = gpu.id\n",
        "              # name of GPU\n",
        "              gpu_name = gpu.name\n",
        "              # get % percentage of GPU usage of that GPU\n",
        "              gpu_load = f\"{gpu.load*100}%\"\n",
        "              # get free memory in MB format\n",
        "              gpu_free_memory = f\"{gpu.memoryFree}MB\"\n",
        "              # get used memory\n",
        "              gpu_used_memory = f\"{gpu.memoryUsed}MB\"\n",
        "              # get total memory\n",
        "              gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
        "              # get GPU temperature in Celsius\n",
        "              gpu_temperature = f\"{gpu.temperature} °C\"\n",
        "              gpu_uuid = gpu.uuid\n",
        "              list_gpus.append((\n",
        "                  gpu_id, gpu_name, gpu_load, gpu_free_memory, gpu_used_memory,\n",
        "                  gpu_total_memory, gpu_temperature, gpu_uuid\n",
        "              ))\n",
        "\n",
        "          print(tabulate(list_gpus, headers=(\"id\", \"name\", \"load\", \"free memory\", \"used memory\", \"total memory\",\n",
        "                                   \"temperature\", \"uuid\")))\n",
        "        cnt += 1\n",
        "\n",
        "    progress_bar.close()\n",
        "    \n",
        "    return losses"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diz-L9GYPNHs"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/last_model_state_dict.pth'))\n",
        "optimizer.load_state_dict(torch.load('drive/MyDrive/last_optimizer_state_dict.pth'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23F1gMpXPjT5"
      },
      "source": [
        "train_losses = []\n",
        "train_perplexities = []\n",
        "\n",
        "for n_epoch in range(1, num_epochs + 1):\n",
        "    \n",
        "    epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n",
        "    mean_train_loss = np.mean(epoch_train_losses)\n",
        "    \n",
        "    train_losses.extend(epoch_train_losses)\n",
        "    train_perplexities.append(np.exp(mean_train_loss))\n",
        "    \n",
        "    \n",
        "    message = f'Epoch: {n_epoch}\\n'\n",
        "    message += f'Train: loss - {mean_train_loss:.4f} | perplexity - {train_perplexities[-1]:.3f}\\n'  \n",
        "    print(message) \n",
        "    torch.save(model.state_dict(), save_last_model_path)\n",
        "    torch.save(optimizer.state_dict(), save_last_optimizer_path)\n",
        "    # 1160/10012"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m9XBwzqPqbs"
      },
      "source": [
        "def speak():\n",
        "  \n",
        "  about = '<bos> Меня Зовут Анна, мне 18 лет. любою котят.'\n",
        "\n",
        "  history = []\n",
        "  print('Начните диалог')\n",
        "\n",
        "  while(True):\n",
        "    message = input()\n",
        "    i = 1\n",
        "    history.append([' <speaker1> ' + message])\n",
        "    input_text = about + ''.join(list(chain(*history))) + ' <fos>' # history[-4:]\n",
        "\n",
        "    inds = tokenizer(input_text, return_tensors='pt')['input_ids'].to(device)\n",
        "    text = model.generate(\n",
        "          input_ids=inds,\n",
        "          repetition_penalty=5.0,\n",
        "          top_k=5, \n",
        "          top_p=0.95, \n",
        "          temperature=1,\n",
        "          max_length= len(inds[0]) + 20,\n",
        "          do_sample=True,\n",
        "          no_repeat_ngram_size=10,\n",
        "          pad_token_id = 50259,\n",
        "          # length_penalty = 0.5\n",
        "          )\n",
        "      \n",
        "    answer = tokenizer.decode(text[0])\n",
        "\n",
        "    history.append([' <speaker2> ' + answer[len(input_text):]])\n",
        "    # print(input_text)\n",
        "    # print(answer)\n",
        "    print(answer[len(input_text):])\n",
        "    # print(history)\n",
        "\n",
        "      \n",
        "      # print(answer[len(input_text):])\n",
        "      # input_text += answer + ' '\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqhxv_DoPqgp"
      },
      "source": [
        "speak()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9i5DLE5Pqlk"
      },
      "source": [
        "arr = [len(generate_example(i)[0]) for i in range(2000, 2420)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ikZnfCsPqqF"
      },
      "source": [
        "print(np.argmax(arr), max(arr))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnCO6GyJj3zN"
      },
      "source": [
        "np.mean(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gabht5jYkEqi"
      },
      "source": [
        "generate_example(115)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTrOzKtzlyIj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}